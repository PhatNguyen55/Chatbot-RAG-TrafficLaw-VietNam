import os
import re
import shutil
from app.services.rag_service import SentenceTransformerEmbeddings
import fitz  # PyMuPDF
import pickle
import json
from pathlib import Path
from typing import List, Dict, Any

from langchain.schema import Document
from sentence_transformers import SentenceTransformer
from langchain_chroma import Chroma
from langchain.text_splitter import RecursiveCharacterTextSplitter

# --- CÃC HÃ€M TIá»†N ÃCH CHO VIá»†C Xá»¬ LÃ VÄ‚N Báº¢N ---

def join_broken_lines(text: str) -> str:
    """Ná»‘i cÃ¡c dÃ²ng bá»‹ ngáº¯t giá»¯a chá»«ng."""
    lines = text.split('\n')
    result_lines = []
    i = 0
    while i < len(lines):
        current_line = lines[i].strip()
        if i + 1 < len(lines):
            next_line = lines[i+1].strip()
            if current_line and next_line and not current_line.endswith(('.', ':', ';', ')')) and next_line[0].islower():
                result_lines.append(current_line + " " + next_line)
                i += 2
                continue
        result_lines.append(current_line)
        i += 1
    return "\n".join(result_lines)

def extract_and_clean_text(pdf_path: str) -> str:
    """TrÃ­ch xuáº¥t vÃ  lÃ m sáº¡ch vÄƒn báº£n tá»« má»™t file PDF."""
    full_text = ""
    try:
        with fitz.open(pdf_path) as doc:
            for page in doc:
                page_text = page.get_text("text")
                page_text = re.sub(r"^\s*\d+\s*$", "", page_text, flags=re.MULTILINE)
                page_text = re.sub(r"^(CÃ”NG BÃO|Dá»° THáº¢O|Luáº­t sá»‘).*?\n", "", page_text, flags=re.IGNORECASE | re.MULTILINE)
                page_text = re.sub(r"KÃ½ bá»Ÿi:.*?\+07:00", "", page_text, flags=re.DOTALL | re.IGNORECASE)
                page_text = re.sub(r'NÆ¡i nháº­n:.*?$(.*?\n)*?(TM\. CHÃNH PHá»¦|KT\. THá»¦ TÆ¯á»šNG).*?$', '', page_text, flags=re.DOTALL | re.MULTILINE)
                page_text = re.sub(r'NgÆ°á»i kÃ½:.*?(\n|$)', '', page_text, flags=re.DOTALL)
                full_text += page_text + "\n"
    except Exception as e:
        print(f"âŒ Lá»—i khi xá»­ lÃ½ PDF '{os.path.basename(pdf_path)}': {e}")
        return ""

    full_text = join_broken_lines(full_text)
    full_text = re.sub(r'[ \t]{2,}', ' ', full_text)
    full_text = re.sub(r'\n{3,}', '\n\n', full_text).strip()
    return full_text

def extract_document_details(filename: str) -> Dict[str, Any]:
    """TrÃ­ch xuáº¥t loáº¡i vÄƒn báº£n, sá»‘ hiá»‡u vÃ  ngÃ y ban hÃ nh tá»« tÃªn file Ä‘á»ƒ lÃ m metadata."""
    details = {"document_type": "VÄƒn báº£n khÃ¡c", "document_number": "KhÃ´ng xÃ¡c Ä‘á»‹nh", "document_date": "KhÃ´ng xÃ¡c Ä‘á»‹nh"}
    filename_lower = filename.lower()
    
    luat_match = re.search(r'luat-(\d+)-(\d{4})-qh\d+', filename_lower)
    if luat_match:
        details["document_type"] = "Luáº­t"
        details["document_number"] = f"{luat_match.group(1)}/{luat_match.group(2)}/QH15"
        details["document_date"] = f"{luat_match.group(2)}"
        return details
    
    nghi_dinh_match = re.search(r'nghi-dinh-(\d+)_(\d{4})_nd-cp', filename_lower)
    if nghi_dinh_match:
        details["document_type"] = "Nghá»‹ Ä‘á»‹nh"
        details["document_number"] = f"{nghi_dinh_match.group(1)}/{nghi_dinh_match.group(2)}/NÄ-CP"
        details["document_date"] = f"{nghi_dinh_match.group(2)}"
        return details
    
    thong_tu_match = re.search(r'thong-tu-(\d+)-(\d{4})-tt-bca', filename_lower)
    if thong_tu_match:
        details["document_type"] = "ThÃ´ng tÆ°"
        details["document_number"] = f"{thong_tu_match.group(1)}/{thong_tu_match.group(2)}/TT-BCA"
        details["document_date"] = f"{thong_tu_match.group(2)}"
        return details
    
    return details

def split_law_document_semantically(cleaned_full_text: str, source_filename: str) -> List[Document]:
    """
    Chia vÄƒn báº£n theo tá»«ng Äiá»u, sau Ä‘Ã³ chia nhá» cÃ¡c Äiá»u quÃ¡ dÃ i má»™t cÃ¡ch thÃ´ng minh.
    """
    doc_details = extract_document_details(source_filename)
    documents = []
    
    current_chuong = ""
    
    # DÃ¹ng text_splitter Ä‘á»ƒ chia nhá» cÃ¡c Äiá»u quÃ¡ dÃ i
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=2000, # Äá»™ dÃ i tá»‘i Ä‘a cá»§a má»™t chunk (tÃ­nh báº±ng kÃ½ tá»±)
        chunk_overlap=200, # CÃ¡c chunk sáº½ gá»‘i lÃªn nhau 200 kÃ½ tá»±
        length_function=len,
        is_separator_regex=False,
        separators=["\n\n", "\n", ". ", ", ", " "], # CÃ¡c dáº¥u ngáº¯t Æ°u tiÃªn
    )

    # TÃ¡ch toÃ n bá»™ vÄƒn báº£n thÃ nh cÃ¡c Äiá»u
    articles = re.split(r'(?=\nChÆ°Æ¡ng\s+[IVXLCDM\d]+|\nÄiá»u\s+\d+)', cleaned_full_text)
    
    for text_block in articles:
        text_block = text_block.strip()
        if not text_block: continue

        chuong_match = re.match(r'ChÆ°Æ¡ng\s+([IVXLCDM\d]+)\.?\s*(.*)', text_block, re.IGNORECASE)
        if chuong_match:
            current_chuong = f"ChÆ°Æ¡ng {chuong_match.group(1)} - {chuong_match.group(2).strip()}"
            continue

        dieu_match = re.match(r'Äiá»u\s+(\d+)\.?:?\s*(.*)', text_block, re.IGNORECASE)
        if not dieu_match: continue

        dieu_so = dieu_match.group(1)
        dieu_ten = dieu_match.group(2).strip().split('\n')[0]
        dieu_title = f"Äiá»u {dieu_so}. {dieu_ten}"
        
        base_metadata = {
            "source_file": source_filename,
            "document_type": doc_details["document_type"],
            "document_number": doc_details["document_number"],
            "chuong": current_chuong,
            "dieu": dieu_title,
            "article_number": dieu_so,
        }
        
        # ThÃªm header ngá»¯ cáº£nh vÃ o ná»™i dung cá»§a Äiá»u
        contextual_header = f"TrÃ­ch tá»«: {doc_details['document_type']} {doc_details['document_number']}, {current_chuong}\n\n"
        content_with_header = contextual_header + text_block

        # Sá»­ dá»¥ng text_splitter Ä‘á»ƒ chia Äiá»u nÃ y thÃ nh cÃ¡c chunk nhá» hÆ¡n náº¿u cáº§n
        chunks = text_splitter.create_documents([content_with_header], metadatas=[base_metadata])
        documents.extend(chunks)
            
    return documents

# --- HÃ€M ÄIá»€U PHá»I CHÃNH ---

def process_and_save_data(pdf_dir: str, all_chunks_path: str, json_all_chunks_path: str, vector_store_path: str):
    """Xá»­ lÃ½ táº¥t cáº£ PDF, lÆ°u danh sÃ¡ch cÃ¡c chunks ra file pickle vÃ  JSON."""
    print("ğŸš€ Báº¯t Ä‘áº§u quÃ¡ trÃ¬nh xá»­ lÃ½ dá»¯ liá»‡u...")
    
    Path(all_chunks_path).parent.mkdir(parents=True, exist_ok=True)
    Path(json_all_chunks_path).parent.mkdir(parents=True, exist_ok=True)
    
    pdf_files = sorted(list(Path(pdf_dir).glob("*.pdf")))
    if not pdf_files:
        print(f"âš ï¸ KhÃ´ng tÃ¬m tháº¥y file PDF nÃ o trong thÆ° má»¥c '{pdf_dir}'")
        return

    all_chunks = []
    for pdf_path in pdf_files:
        print(f"âš™ï¸ Äang xá»­ lÃ½ file: {pdf_path.name}")
        cleaned_text = extract_and_clean_text(str(pdf_path))
        if cleaned_text:
            chunks = split_law_document_semantically(cleaned_text, pdf_path.name)
            all_chunks.extend(chunks)
            print(f"âœ… ÄÃ£ chia thÃ nh cÃ´ng {len(chunks)} chunks tá»« file {pdf_path.name}.")

     # --- BÆ¯á»šC Lá»ŒC CHUNKS RÃC ---
    print(f"\nLá»c {len(all_chunks)} chunks thÃ´...")
    final_chunks = [
        chunk for chunk in all_chunks 
        if len(chunk.page_content.split()) > 8 # Chá»‰ giá»¯ chunk cÃ³ hÆ¡n 8 tá»«
    ]
    print(f"âœ… ÄÃ£ lá»c, cÃ²n láº¡i {len(final_chunks)} chunks cháº¥t lÆ°á»£ng.")

    # --- LÆ¯U CÃC CHUNKS ÄÃƒ Lá»ŒC ---
    with open(all_chunks_path, "wb") as f:
        pickle.dump(final_chunks, f)
    
    json_data = [{"page_content": chunk.page_content, "metadata": chunk.metadata} for chunk in final_chunks]
    with open(json_all_chunks_path, "w", encoding="utf-8") as f:
        json.dump(json_data, f, ensure_ascii=False, indent=4)
        
    print(f"\nÄÃ£ lÆ°u {len(final_chunks)} chunks vÃ o '{all_chunks_path}' vÃ  '{json_all_chunks_path}'.")

    # ====================================================================
    # <<< LOGIC Táº O VÃ€ LÆ¯U TRá»® CHROMA DB VÄ¨NH VIá»„N >>>
    # ====================================================================
    print("\nâš™ï¸ Báº¯t Ä‘áº§u táº¡o Vector Store (ChromaDB)...")

    # 1. XÃ³a thÆ° má»¥c vector store cÅ© Ä‘á»ƒ Ä‘áº£m báº£o táº¡o má»›i hoÃ n toÃ n
    if os.path.exists(vector_store_path):
        print(f"   - TÃ¬m tháº¥y thÆ° má»¥c Vector Store cÅ©. Äang xÃ³a: {vector_store_path}")
        shutil.rmtree(vector_store_path)
    
    # 2. Táº£i model embedding (chá»‰ cáº§n cho bÆ°á»›c nÃ y)
    # Táº£i model embedding tá»« local
    embedding_model_path = "models/bkai-foundation-models_vietnamese-bi-encoder"
    print(f"   - Táº£i embedding model tá»«: {embedding_model_path}")
    embedding_model = SentenceTransformer(embedding_model_path)
    langchain_embedding = SentenceTransformerEmbeddings(embedding_model)

    # 3. Táº¡o ChromaDB tá»« cÃ¡c chunks vÃ  lÆ°u nÃ³ vÃ o Ä‘Ä©a
    print(f"   - Äang embedding vÃ  táº¡o database táº¡i: {vector_store_path}")
    vector_store = Chroma.from_documents(
        documents=final_chunks,
        embedding=langchain_embedding,
        persist_directory=vector_store_path # <-- Chá»‰ Ä‘á»‹nh thÆ° má»¥c lÆ°u trá»¯
    )
    
    # DÃ²ng nÃ y khÃ´ng cáº§n thiáº¿t vá»›i phiÃªn báº£n Chroma má»›i, from_documents Ä‘Ã£ tá»± lÆ°u
    # vector_store.persist() 

    print("âœ… ÄÃ£ táº¡o vÃ  lÆ°u trá»¯ thÃ nh cÃ´ng Vector Store trÃªn Ä‘Ä©a!")
    print("\nğŸ‰ HOÃ€N Táº¤T TOÃ€N Bá»˜ QUÃ TRÃŒNH Xá»¬ LÃ Dá»® LIá»†U!")

if __name__ == "__main__":
    import sys
    sys.path.append(os.getcwd())
    from app.core.config import settings

    print("Cháº¡y data_loader nhÆ° má»™t script Ä‘á»™c láº­p...")
    process_and_save_data(
        pdf_dir=settings.PDF_DIRECTORY,
        all_chunks_path=settings.ALL_CHUNKS_PATH,
        json_all_chunks_path=settings.ALL_CHUNKS_JSON_PATH,
        vector_store_path=settings.VECTOR_STORE_DIRECTORY
    )