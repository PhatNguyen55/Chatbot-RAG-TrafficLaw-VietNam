import os
import re
import fitz  # PyMuPDF
import pickle
from pathlib import Path
from typing import List, Dict, Any

from langchain.schema import Document

# --- C√ÅC H√ÄM TI·ªÜN √çCH CHO VI·ªÜC X·ª¨ L√ù VƒÇN B·∫¢N ---
# ƒê√¢y l√† c√°c h√†m ƒë√£ ƒë∆∞·ª£c ki·ªÉm ch·ª©ng t·ª´ Colab

def join_broken_lines(text: str) -> str:
    """N·ªëi c√°c d√≤ng b·ªã ng·∫Øt gi·ªØa ch·ª´ng."""
    lines = text.split('\n')
    result_lines = []
    i = 0
    while i < len(lines):
        current_line = lines[i].strip()
        if i + 1 < len(lines):
            next_line = lines[i+1].strip()
            # ƒêi·ªÅu ki·ªán n·ªëi d√≤ng: d√≤ng hi·ªán t·∫°i kh√¥ng k·∫øt th√∫c b·∫±ng d·∫•u c√¢u v√† d√≤ng ti·∫øp theo b·∫Øt ƒë·∫ßu b·∫±ng ch·ªØ th∆∞·ªùng.
            if current_line and next_line and not current_line.endswith(('.', ':', ';', ')')) and next_line[0].islower():
                result_lines.append(current_line + " " + next_line)
                i += 2
                continue
        result_lines.append(current_line)
        i += 1
    return "\n".join(result_lines)


def extract_and_clean_text(pdf_path: str) -> str:
    """Tr√≠ch xu·∫•t v√† l√†m s·∫°ch vƒÉn b·∫£n t·ª´ m·ªôt file PDF."""
    full_text = ""
    try:
        with fitz.open(pdf_path) as doc:
            for page in doc:
                page_text = page.get_text("text")
                # Lo·∫°i b·ªè header/footer c∆° b·∫£n (s·ªë trang ƒë·ª©ng m·ªôt m√¨nh)
                page_text = re.sub(r"^\s*\d+\s*$", "", page_text, flags=re.MULTILINE)
                # Lo·∫°i b·ªè c√°c d√≤ng ti√™u ƒë·ªÅ kh√¥ng c·∫ßn thi·∫øt
                page_text = re.sub(r"^(C√îNG B√ÅO|D·ª∞ TH·∫¢O|Lu·∫≠t s·ªë).*?\n", "", page_text, flags=re.IGNORECASE | re.MULTILINE)
                # Lo·∫°i b·ªè th√¥ng tin ch·ªØ k√Ω s·ªë
                page_text = re.sub(r"K√Ω b·ªüi:.*?\+07:00", "", page_text, flags=re.DOTALL | re.IGNORECASE)
                full_text += page_text + "\n"
    except Exception as e:
        print(f"‚ùå L·ªói khi x·ª≠ l√Ω PDF '{os.path.basename(pdf_path)}': {e}")
        return ""

    full_text = join_broken_lines(full_text)
    full_text = re.sub(r'[ \t]{2,}', ' ', full_text)
    full_text = re.sub(r'\n{3,}', '\n\n', full_text).strip()
    return full_text


def extract_document_details(filename: str) -> Dict[str, Any]:
    """Tr√≠ch xu·∫•t lo·∫°i vƒÉn b·∫£n v√† s·ªë hi·ªáu t·ª´ t√™n file ƒë·ªÉ l√†m metadata."""
    details = {"document_type": "VƒÉn b·∫£n kh√°c", "document_number": "Kh√¥ng x√°c ƒë·ªãnh"}
    filename_lower = filename.lower()
    
    nghi_dinh_match = re.search(r'nghi-dinh-(\d+)-(\d{4})', filename_lower)
    if nghi_dinh_match:
        details["document_type"] = "Ngh·ªã ƒë·ªãnh"
        details["document_number"] = f"{nghi_dinh_match.group(1)}/{nghi_dinh_match.group(2)}/Nƒê-CP"
        return details

    luat_match = re.search(r'luat.*(\d{4})', filename_lower)
    if luat_match:
        details["document_type"] = "Lu·∫≠t"
        details["document_number"] = f"Lu·∫≠t nƒÉm {luat_match.group(1)}"
        return details
        
    return details


def split_law_document_semantically(cleaned_full_text: str, source_filename: str) -> List[Document]:
    """
    H√†m c·ªët l√µi: Chia vƒÉn b·∫£n ph√°p quy theo c·∫•u tr√∫c ng·ªØ nghƒ©a (Ch∆∞∆°ng -> ƒêi·ªÅu) v√† g√°n metadata chi ti·∫øt.
    """
    if not cleaned_full_text:
        return []

    doc_details = extract_document_details(source_filename)
    
    documents = []
    current_chuong = "Kh√¥ng x√°c ƒë·ªãnh"
    current_muc = None

    # T√°ch vƒÉn b·∫£n th√†nh c√°c kh·ªëi l·ªõn b·∫Øt ƒë·∫ßu b·∫±ng "Ch∆∞∆°ng", "M·ª•c" ho·∫∑c "ƒêi·ªÅu"
    articles = re.split(r'(?=\nCh∆∞∆°ng\s+[IVXLCDM\d]+|\nM·ª•c\s+\d+|\nƒêi·ªÅu\s+\d+)', cleaned_full_text)
    articles = [article.strip() for article in articles if article.strip()]

    for article_text in articles:
        # C·∫≠p nh·∫≠t th√¥ng tin Ch∆∞∆°ng, M·ª•c hi·ªán t·∫°i
        chuong_match = re.match(r'Ch∆∞∆°ng\s+([IVXLCDM\d]+)\n(.*?)\n', article_text, re.IGNORECASE)
        muc_match = re.match(r'M·ª•c\s+(\d+)\n(.*?)\n', article_text, re.IGNORECASE)
        dieu_match = re.match(r'ƒêi·ªÅu\s+(\d+)\.?\s*(.*)', article_text, re.IGNORECASE)

        if chuong_match:
            chuong_so = chuong_match.group(1)
            chuong_ten = chuong_match.group(2).strip()
            current_chuong = f"Ch∆∞∆°ng {chuong_so} - {chuong_ten}"
            current_muc = None # Reset M·ª•c khi g·∫∑p Ch∆∞∆°ng m·ªõi
            continue 
        
        if muc_match:
            muc_so = muc_match.group(1)
            muc_ten = muc_match.group(2).strip()
            current_muc = f"M·ª•c {muc_so} - {muc_ten}"
            continue
            
        if dieu_match:
            # N·∫øu l√† m·ªôt ƒêi·ªÅu, tr√≠ch xu·∫•t metadata v√† t·∫°o Document
            dieu_so = dieu_match.group(1)
            dieu_ten = dieu_match.group(2).strip().split('\n')[0]
            dieu_title = f"ƒêi·ªÅu {dieu_so}. {dieu_ten}"

            # M·ªói ƒêi·ªÅu l√† m·ªôt Document. Ch√∫ng ta kh√¥ng chia nh·ªè h∆°n ·ªü b∆∞·ªõc n√†y.
            # Vi·ªác chia nh·ªè h∆°n s·∫Ω do c√°c l·ªõp splitter c·ªßa LangChain x·ª≠ l√Ω sau n·∫øu c·∫ßn.
            # ·ªû ƒë√¢y, m·ªói ƒêi·ªÅu l√† m·ªôt chunk.
            
            metadata = {
                "source_file": source_filename,
                "document_type": doc_details["document_type"],
                "document_number": doc_details["document_number"],
                "chuong": current_chuong,
                "dieu": dieu_title,
            }
            if current_muc:
                metadata["muc"] = current_muc
            
            documents.append(Document(page_content=article_text.strip(), metadata=metadata))
    
    return documents


# --- H√ÄM ƒêI·ªÄU PH·ªêI CH√çNH ---

def process_and_save_data(pdf_dir: str, save_path: str):
    """
    H√†m ch√≠nh ƒë·ªÉ x·ª≠ l√Ω t·∫•t c·∫£ PDF v√† l∆∞u danh s√°ch c√°c chunks (Documents) ra file pickle.
    """
    print("üöÄ B·∫Øt ƒë·∫ßu qu√° tr√¨nh x·ª≠ l√Ω d·ªØ li·ªáu...")
    
    # T·∫°o th∆∞ m·ª•c cha c·ªßa save_path n·∫øu ch∆∞a c√≥
    Path(save_path).parent.mkdir(parents=True, exist_ok=True)
    
    pdf_files = sorted(list(Path(pdf_dir).glob("*.pdf")))
    if not pdf_files:
        print(f"‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y file PDF n√†o trong th∆∞ m·ª•c '{pdf_dir}'")
        return

    all_chunks = []
    for pdf_path in pdf_files:
        print(f"‚öôÔ∏è ƒêang x·ª≠ l√Ω file: {pdf_path.name}")
        cleaned_text = extract_and_clean_text(str(pdf_path))
        if cleaned_text:
            chunks = split_law_document_semantically(cleaned_text, pdf_path.name)
            all_chunks.extend(chunks)
            print(f"‚úÖ ƒê√£ chia th√†nh c√¥ng {len(chunks)} chunks t·ª´ file {pdf_path.name}.")

    # L∆∞u l·∫°i to√†n b·ªô chunks ƒë√£ x·ª≠ l√Ω v√†o m·ªôt file duy nh·∫•t
    with open(save_path, "wb") as f:
        pickle.dump(all_chunks, f)
        
    print(f"\nüéâ HO√ÄN T·∫§T! ƒê√£ l∆∞u t·ªïng c·ªông {len(all_chunks)} chunks v√†o '{save_path}'")


# --- KH·ªêI ƒê·ªÇ CH·∫†Y TR·ª∞C TI·∫æP SCRIPT N√ÄY ---

if __name__ == "__main__":
    # Import settings ƒë·ªÉ l·∫•y ƒë∆∞·ªùng d·∫´n ƒë√£ c·∫•u h√¨nh
    # C·∫ßn m·ªôt ch√∫t thay ƒë·ªïi ƒë·ªÉ script c√≥ th·ªÉ ch·∫°y ƒë·ªôc l·∫≠p
    import sys
    sys.path.append(os.getcwd())
    from app.core.config import settings

    print("Ch·∫°y data_loader nh∆∞ m·ªôt script ƒë·ªôc l·∫≠p...")
    
    # Ch·∫°y h√†m x·ª≠ l√Ω ch√≠nh
    process_and_save_data(
        pdf_dir=settings.PDF_DIRECTORY,
        save_path=settings.ALL_CHUNKS_PATH
    )