import os
import pickle
import re
import numpy as np
import torch
# import sentencepiece
# import google.generativeai as genai
from typing import List, Dict, Any

from langchain.schema import Document
from langchain.prompts import PromptTemplate
from langchain.chains import RetrievalQA
from langchain_chroma import Chroma
from langchain_core.embeddings import Embeddings
from langchain_core.retrievers import BaseRetriever
from langchain_core.callbacks import CallbackManagerForRetrieverRun
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.chains import ConversationalRetrievalChain
from langchain.memory import ConversationBufferMemory

from sentence_transformers import SentenceTransformer, CrossEncoder
from rank_bm25 import BM25Okapi
# from transformers import AutoTokenizer, AutoModel

from app.core.config import settings

# --- C√ÅC CLASS V√Ä BI·∫æN TO√ÄN C·ª§C (ƒë√£ ƒë∆∞·ª£c ki·ªÉm ch·ª©ng t·ª´ Colab) ---

class SentenceTransformerEmbeddings(Embeddings):
    """Wrapper cho SentenceTransformer ƒë·ªÉ t∆∞∆°ng th√≠ch v·ªõi LangChain."""
    def __init__(self, model):
        super().__init__()
        self.model = model

    def embed_documents(self, texts: List[str]) -> List[List[float]]:
        # Kh√¥ng hi·ªÉn th·ªã progress bar khi ch·∫°y tr√™n server
        return self.model.encode(texts, convert_to_tensor=False, show_progress_bar=False).tolist()

    def embed_query(self, text: str) -> List[float]:
        return self.model.encode([text], convert_to_tensor=False)[0].tolist()
    

class HybridRerankingRetriever(BaseRetriever):
    """Retriever lai gh√©p, k·∫øt h·ª£p vector v√† keyword, sau ƒë√≥ re-rank."""
    vector_store: Chroma
    bm25_searcher: BM25Okapi
    all_docs: List[Document]
    reranker: CrossEncoder
    top_n_vector: int = 7
    top_n_keyword: int = 7
    top_k_final: int = 5

    def _get_relevant_documents(
        self, query: str, *, run_manager: CallbackManagerForRetrieverRun, where_filter: Dict[str, Any] = None
    ) -> List[Document]:
        
        # 1. Vector Search v·ªõi b·ªô l·ªçc metadata (n·∫øu c√≥)
        print(f"DEBUG: Performing vector search with filter: {where_filter}")
        if where_filter:
            vector_docs = self.vector_store.similarity_search(query, k=self.top_n_vector, filter=where_filter)
        else:
            vector_docs = self.vector_store.similarity_search(query, k=self.top_n_vector)
        
        # 2. Keyword Search (BM25)
        tokenized_query = query.split(" ")
        bm25_scores = self.bm25_searcher.get_scores(tokenized_query)
        # L·∫•y c√°c index c√≥ score > 0 ƒë·ªÉ tr√°nh k·∫øt qu·∫£ kh√¥ng li√™n quan
        top_n_indices = np.argsort(bm25_scores)[::-1][:self.top_n_keyword]
        bm25_docs = [self.all_docs[i] for i in top_n_indices if bm25_scores[i] > 0]
        
        # 3. K·∫øt h·ª£p v√† lo·∫°i b·ªè tr√πng l·∫∑p
        combined_docs_dict = {doc.page_content: doc for doc in vector_docs}
        for doc in bm25_docs:
            if doc.page_content not in combined_docs_dict:
                combined_docs_dict[doc.page_content] = doc
        
        combined_docs = list(combined_docs_dict.values())

        if not combined_docs:
            return []
        
        # 4. Re-ranking
        sentence_pairs = [[query, doc.page_content] for doc in combined_docs]
        scores = self.reranker.predict(sentence_pairs, show_progress_bar=False)
        
        scored_docs = sorted(zip(scores, combined_docs), key=lambda x: x[0], reverse=True)
        
        reranked_docs = [doc for score, doc in scored_docs][:self.top_k_final]
        
        return reranked_docs

QUERY_EXPANSION_MAP = {
    "v∆∞·ª£t ƒë√®n ƒë·ªè": "kh√¥ng ch·∫•p h√†nh hi·ªáu l·ªánh c·ªßa ƒë√®n t√≠n hi·ªáu giao th√¥ng",
    "v∆∞·ª£t ƒë√®n v√†ng": "kh√¥ng ch·∫•p h√†nh hi·ªáu l·ªánh ƒë√®n t√≠n hi·ªáu",
    "kh√¥ng ƒë·ªôi m≈© b·∫£o hi·ªÉm": "kh√¥ng ƒë·ªôi m≈© b·∫£o hi·ªÉm ho·∫∑c ƒë·ªôi m≈© kh√¥ng c√†i quai ƒë√∫ng quy c√°ch",
    "say x·ªân": "c√≥ n·ªìng ƒë·ªô c·ªìn trong m√°u ho·∫∑c h∆°i th·ªü",
    "u·ªëng r∆∞·ª£u bia l√°i xe": "ƒëi·ªÅu khi·ªÉn ph∆∞∆°ng ti·ªán c√≥ n·ªìng ƒë·ªô c·ªìn",
    "ƒëi sai l√†n": "ƒëi kh√¥ng ƒë√∫ng l√†n ƒë∆∞·ªùng ho·∫∑c ph·∫ßn ƒë∆∞·ªùng quy ƒë·ªãnh",
    "l√°i xe qu√° t·ªëc ƒë·ªô": "ƒëi·ªÅu khi·ªÉn xe ch·∫°y qu√° t·ªëc ƒë·ªô cho ph√©p",
    "b·ªã ph·∫°t ngu·ªôi": "x·ª≠ ph·∫°t qua h·ªá th·ªëng gi√°m s√°t t·ª± ƒë·ªông",
    "kh√¥ng b·∫±ng l√°i": "kh√¥ng c√≥ gi·∫•y ph√©p l√°i xe",
    "kh√¥ng c√≥ b·∫±ng l√°i": "kh√¥ng c√≥ gi·∫•y ph√©p l√°i xe",
}

def expand_query(query: str) -> str:
    """M·ªü r·ªông c√¢u h·ªèi b·∫±ng c√°ch thay th·∫ø thu·∫≠t ng·ªØ ph·ªï th√¥ng b·∫±ng thu·∫≠t ng·ªØ ph√°p l√Ω."""
    # D√πng lower() ƒë·ªÉ b·∫Øt ƒë∆∞·ª£c nhi·ªÅu tr∆∞·ªùng h·ª£p h∆°n
    lower_query = query.lower()
    for key, legal_term in QUERY_EXPANSION_MAP.items():
        if key in lower_query:
            # Tr·∫£ v·ªÅ c·∫£ hai ƒë·ªÉ tƒÉng kh·∫£ nƒÉng t√¨m ki·∫øm
            return f"{query} ({legal_term})" 
    return query
    
def extract_query_details(query: str) -> dict:
        """D√πng regex ƒë·ªÉ t√¨m ki·∫øm s·ªë hi·ªáu vƒÉn b·∫£n v√† s·ªë ƒëi·ªÅu trong c√¢u h·ªèi."""
        details = {}
        # V√≠ d·ª•: "ngh·ªã ƒë·ªãnh 100", "lu·∫≠t 35/2024", "th√¥ng t∆∞ 79"
        doc_match = re.search(r'(lu·∫≠t|ngh·ªã ƒë·ªãnh|th√¥ng t∆∞)\s*(\d+/?\d*)', query, re.IGNORECASE)
        if doc_match:
            doc_type = doc_match.group(1).lower()
            doc_num = doc_match.group(2)
            if "lu·∫≠t" in doc_type:
                details['document_type'] = "Lu·∫≠t"
            elif "ngh·ªã ƒë·ªãnh" in doc_type:
                details['document_type'] = "Ngh·ªã ƒë·ªãnh"
            elif "th√¥ng t∆∞" in doc_type:
                details['document_type'] = "Th√¥ng t∆∞"
            # T√¨m ki·∫øm m·ªôt ph·∫ßn c·ªßa document_number
            details['document_number_partial'] = doc_num

        # V√≠ d·ª•: "ƒëi·ªÅu 9", "theo ƒëi·ªÅu 15"
        article_match = re.search(r'ƒëi·ªÅu\s+(\d+)', query, re.IGNORECASE)
        if article_match:
            details['article_number'] = article_match.group(1)
            
        return details
    
# Prompt Template ƒë∆∞·ª£c thi·∫øt k·∫ø k·ªπ l∆∞·ª°ng
CONDENSE_QUESTION_PROMPT_TEMPLATE = """D·ª±a v√†o ƒëo·∫°n h·ªôi tho·∫°i d∆∞·ªõi ƒë√¢y v√† m·ªôt c√¢u h·ªèi ti·∫øp theo, h√£y di·ªÖn gi·∫£i c√¢u h·ªèi ti·∫øp theo th√†nh m·ªôt c√¢u h·ªèi ƒë·ªôc l·∫≠p, ƒë·∫ßy ƒë·ªß b·∫±ng ti·∫øng Vi·ªát.

L·ªãch s·ª≠ tr√≤ chuy·ªán:
{chat_history}

C√¢u h·ªèi ti·∫øp theo: {question}
C√¢u h·ªèi ƒë·ªôc l·∫≠p:"""

CONDENSE_QUESTION_PROMPT = PromptTemplate.from_template(CONDENSE_QUESTION_PROMPT_TEMPLATE)

RAG_PROMPT_TEMPLATE = """B·∫°n l√† LawBot, m·ªôt chuy√™n gia AI v·ªÅ Lu·∫≠t Giao th√¥ng ƒê∆∞·ªùng b·ªô Vi·ªát Nam. Nhi·ªám v·ª• c·ªßa b·∫°n l√† tr·∫£ l·ªùi c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng m·ªôt c√°ch ch√≠nh x√°c, c√≥ cƒÉn c·ª© ph√°p l√Ω r√µ r√†ng, ch·ªâ d·ª±a v√†o NG·ªÆ C·∫¢NH ƒë∆∞·ª£c cung c·∫•p.

---
üéØ **M·ª§C TI√äU C√ÇU TR·∫¢ L·ªúI:**
- Tr√¨nh b√†y r√µ r√†ng, ƒë√∫ng lu·∫≠t, c√≥ c·∫•u tr√∫c.
- Ng·∫Øn g·ªçn, d·ªÖ hi·ªÉu, ph√π h·ª£p v·ªõi ng∆∞·ªùi d√¢n ph·ªï th√¥ng.
- C√≥ tr√≠ch d·∫´n ƒëi·ªÅu lu·∫≠t c·ª• th·ªÉ t·ª´ metadata c√≥ trong NG·ªÆ C·∫¢NH.

---
üß† **QUY TR√åNH SUY LU·∫¨N:**
1.  **Ph√¢n t√≠ch c√¢u h·ªèi**: Hi·ªÉu ƒë√∫ng y√™u c·∫ßu c·ªßa ng∆∞·ªùi d√πng.
2.  **T√¨m ki·∫øm trong NG·ªÆ C·∫¢NH**: T√¨m t·∫•t c·∫£ th√¥ng tin li√™n quan ƒë·∫øn h√†nh vi vi ph·∫°m, m·ª©c ph·∫°t, c√°c t√¨nh hu·ªëng kh√°c nhau (v√≠ d·ª•: cho √¥ t√¥, cho xe m√°y, g√¢y tai n·∫°n).
3.  **C·∫•u tr√∫c h√≥a c√¢u tr·∫£ l·ªùi**: N·∫øu t√¨m ƒë∆∞·ª£c ƒë·ªß th√¥ng tin, tr√¨nh b√†y theo c·∫•u tr√∫c sau:
    - B·∫Øt ƒë·∫ßu b·∫±ng m·ªôt c√¢u t√≥m t·∫Øt chung.
    - D√πng g·∫°ch ƒë·∫ßu d√≤ng ho·∫∑c ti√™u ƒë·ªÅ cho t·ª´ng lo·∫°i ph∆∞∆°ng ti·ªán (`#### üöó V·ªõi xe √¥ t√¥:`).
    - V·ªõi m·ªói ph∆∞∆°ng ti·ªán, ghi r√µ: M·ª©c ph·∫°t, h√¨nh ph·∫°t b·ªï sung.
    - **B·∫ÆT BU·ªòC** tr√≠ch d·∫´n ngu·ªìn cho m·ªói th√¥ng tin b·∫±ng c√°ch s·ª≠ d·ª•ng th√¥ng tin c√≥ s·∫µn trong NG·ªÆ C·∫¢NH. V√≠ d·ª•: `(theo ƒêi·ªÅu 7 c·ªßa Ngh·ªã ƒë·ªãnh 168/2024/Nƒê-CP)`.
4.  **N·∫øu kh√¥ng ƒë·ªß th√¥ng tin**: Tr·∫£ l·ªùi l·ªãch s·ª±: "D·ª±a tr√™n c√°c t√†i li·ªáu ƒë∆∞·ª£c cung c·∫•p, t√¥i kh√¥ng t√¨m th·∫•y th√¥ng tin c·ª• th·ªÉ v·ªÅ [ch·ªß ƒë·ªÅ]."
5.  **Tuy·ªát ƒë·ªëi KH√îNG b·ªãa ƒë·∫∑t**.

---
**NG·ªÆ C·∫¢NH:**
{context}
---
**C√ÇU H·ªéI:** {question}
---
**C√ÇU TR·∫¢ L·ªúI (tu√¢n th·ªß to√†n b·ªô h∆∞·ªõng d·∫´n tr√™n):**
"""
RAG_PROMPT = PromptTemplate(template=RAG_PROMPT_TEMPLATE, input_variables=["context", "question"])


# --- CLASS RAG SERVICE CH√çNH ---

class RAGService:
    def __init__(self):
        # self.qa_chain = None
        self.conversation_chain = None
        self.is_ready = False
        print("Initializing RAG Service...")

    def load(self):
        """
        H√†m c·ªët l√µi: T·∫£i t·∫•t c·∫£ model, index v√† x√¢y d·ª±ng QA chain.
        H√†m n√†y ƒë∆∞·ª£c g·ªçi m·ªôt l·∫ßn khi server kh·ªüi ƒë·ªông.
        """
        try:
            # 1. Ki·ªÉm tra xem d·ªØ li·ªáu ƒë√£ ƒë∆∞·ª£c x·ª≠ l√Ω ch∆∞a
            if not os.path.exists(settings.ALL_CHUNKS_PATH):
                raise FileNotFoundError(f"File d·ªØ li·ªáu '{settings.ALL_CHUNKS_PATH}' kh√¥ng t·ªìn t·∫°i. "
                                        "Vui l√≤ng ch·∫°y 'python -m app.services.data_loader' tr∆∞·ªõc.")

            print("Loading RAG components...")
            device = 'cuda' if torch.cuda.is_available() else 'cpu'
            print(f"S·ª≠ d·ª•ng thi·∫øt b·ªã: {device}")

             # 2. T·∫£i model EMBEDDING t·ª´ th∆∞ m·ª•c local
            embedding_model_folder = "bkai-foundation-models_vietnamese-bi-encoder"
            embedding_model_path = os.path.join(settings.MODELS_DIRECTORY, embedding_model_folder)
            
            if not os.path.exists(embedding_model_path):
                raise FileNotFoundError(f"Th∆∞ m·ª•c model embedding kh√¥ng t·ªìn t·∫°i: {embedding_model_path}")
            
            print(f"Loading embedding model from: {embedding_model_path}")
            embedding_model = SentenceTransformer(embedding_model_path, device=device)

            # 3. T·∫£i model RERANKER t·ª´ th∆∞ m·ª•c local
            reranker_model_folder = "AITeamVN_Vietnamese_Reranker"
            reranker_model_path = os.path.join(settings.MODELS_DIRECTORY, reranker_model_folder)

            if not os.path.exists(reranker_model_path):
                raise FileNotFoundError(f"Th∆∞ m·ª•c model reranker kh√¥ng t·ªìn t·∫°i: {reranker_model_path}")

            print(f"Loading reranker model from: {reranker_model_path}")
            self.reranker = CrossEncoder(reranker_model_path, device=device, max_length=512)
            
            # 3. Kh·ªüi t·∫°o LLM
            self.llm = ChatGoogleGenerativeAI(
                model="models/gemini-1.5-flash-latest",
                temperature=0.1,
                convert_system_message_to_human=True,
                google_api_key=settings.GOOGLE_API_KEY
            )

            
             # 4. T·∫£i d·ªØ li·ªáu chunks (ch·ªâ c·∫ßn cho BM25)
            with open(settings.ALL_CHUNKS_PATH, "rb") as f:
                all_chunks = pickle.load(f)

            # 5. X√¢y d·ª±ng c√°c index
            
            # 5a. T·∫£i ChromaDB t·ª´ ƒëƒ©a
            print(f"Loading Vector Store from disk: {settings.VECTOR_STORE_DIRECTORY}")
            langchain_embedding = SentenceTransformerEmbeddings(embedding_model) # V·∫´n c·∫ßn h√†m embedding ƒë·ªÉ load
            
            # Thay v√¨ Chroma.from_documents, ch√∫ng ta kh·ªüi t·∫°o Chroma v√† tr·ªè ƒë·∫øn th∆∞ m·ª•c ƒë√£ l∆∞u
            self.vector_store = Chroma(
                persist_directory=settings.VECTOR_STORE_DIRECTORY,
                embedding_function=langchain_embedding
            )
            print(f"‚úÖ Vector Store loaded successfully with {self.vector_store._collection.count()} documents.")

            # 5b. T·∫°o BM25 Index (v·∫´n t·∫°o trong RAM khi kh·ªüi ƒë·ªông)
            print("Creating BM25 Index in memory...")
            corpus = [chunk.page_content for chunk in all_chunks]
            tokenized_corpus = [doc.split(" ") for doc in corpus]
            bm25_index = BM25Okapi(tokenized_corpus)
            
            # 6. T·∫°o retriever lai gh√©p
            hybrid_retriever = HybridRerankingRetriever(
                vector_store=self.vector_store,
                bm25_searcher=bm25_index,
                all_docs=all_chunks,
                reranker=self.reranker
            )

             # Chain n√†y s·∫Ω l√† "b·ªô n√£o" ch√≠nh, nh∆∞ng ch√∫ng ta s·∫Ω kh√¥ng d√πng n√≥ tr·ª±c ti·∫øp
            # m√† s·∫Ω d√πng c√°c th√†nh ph·∫ßn c·ªßa n√≥.
            memory = ConversationBufferMemory(
                memory_key='chat_history', return_messages=True, output_key='answer'
            )
            self.conversation_chain = ConversationalRetrievalChain.from_llm(
                llm=self.llm,
                retriever=hybrid_retriever,
                memory=memory,
                return_source_documents=True,
                condense_question_prompt=CONDENSE_QUESTION_PROMPT,
                combine_docs_chain_kwargs={"prompt": RAG_PROMPT}
            )
            
            self.is_ready = True
            print("‚úÖ RAG Service is fully loaded and ready.")
        except Exception as e:
            print(f"‚ùå Failed to load RAG Service: {e}")
            self.is_ready = False

    def ask(self, question: str, chat_history: list = []) -> Dict[str, Any]:
        """
        H√†m x·ª≠ l√Ω c√¢u h·ªèi, s·ª≠ d·ª•ng tr·ª±c ti·∫øp ConversationalRetrievalChain.
        """
        if not self.is_ready or not self.conversation_chain:
            return {"answer": "H·ªá th·ªëng ch∆∞a s·∫µn s√†ng...", "sources": []}
        
        try:
            # Logic x·ª≠ l√Ω meta-question v·∫´n h·ªØu √≠ch
            meta_questions = ["b·∫°n l√† ai", "b·∫°n t√™n g√¨"]
            if any(q in question.lower() for q in meta_questions):
                return {"answer": "T√¥i l√† LawBot, m·ªôt tr·ª£ l√Ω AI chuy√™n v·ªÅ Lu·∫≠t Giao th√¥ng...", "sources": []}

            # --- B∆Ø·ªöC 2: M·ªü r·ªông c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng ---
            expanded_question = expand_query(question)
            print(f"INFO: Expanded Query: '{expanded_question}'")
            
            # --- B∆Ø·ªöC 3: T√°i c·∫•u tr√∫c c√¢u h·ªèi d·ª±a tr√™n l·ªãch s·ª≠ ---
            # Ch√∫ng ta s·∫Ω g·ªçi ri√™ng ph·∫ßn "t·∫°o c√¢u h·ªèi" c·ªßa chain
            _inputs = {"question": expanded_question, "chat_history": chat_history}
            result_from_generator = self.conversation_chain.question_generator.invoke(_inputs)
            # L·∫•y gi√° tr·ªã t·ª´ key 'text' thay v√¨ g√°n c·∫£ dictionary
            standalone_question = result_from_generator.get('text', expanded_question) 
            
            print(f"INFO: Standalone question: '{standalone_question}'")
            # --- B∆Ø·ªöC 4: Tr√≠ch xu·∫•t metadata v√† L·ªçc ---
            query_details = extract_query_details(standalone_question)
            where_filter = {}
            if query_details.get('document_number_partial'):
                where_filter['document_number'] = {"$contains": query_details['document_number_partial']}
            if query_details.get('article_number'):
                where_filter['article_number'] = query_details['article_number']
            
            final_filter = where_filter if where_filter else None
            
            # G·ªçi retriever v·ªõi c√¢u h·ªèi ƒë·ªôc l·∫≠p v√† b·ªô l·ªçc
            retriever = self.conversation_chain.retriever
            docs = retriever.invoke(standalone_question, config={"configurable": {"where_filter": final_filter}})

            # --- B∆Ø·ªöC 5: G·ªçi chain sinh c√¢u tr·∫£ l·ªùi ---
            # Ch√∫ng ta g·ªçi ri√™ng ph·∫ßn "k·∫øt h·ª£p t√†i li·ªáu" c·ªßa chain
            new_inputs = {"question": standalone_question, "input_documents": docs}
            answer = self.conversation_chain.combine_docs_chain.invoke(new_inputs)
            
            sources = [doc.metadata for doc in answer.get("input_documents", [])]

            return {"answer": answer.get("output_text"), "sources": sources}

        except Exception as e:
            print(f"ERROR in ask function: {e}")
            import traceback
            traceback.print_exc()
            return {"answer": "ƒê√£ c√≥ l·ªói nghi√™m tr·ªçng x·∫£y ra...", "sources": []}

# T·∫°o m·ªôt instance duy nh·∫•t (singleton) ƒë·ªÉ import v√† s·ª≠ d·ª•ng trong to√†n b·ªô ·ª©ng d·ª•ng
rag_service = RAGService()